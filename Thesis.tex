\documentclass[conference]{IEEEtran}

\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}
\usepackage[ruled,vlined]{algorithm2e}

\algblock{ParFor}{EndParFor}
% customising the new block
\algnewcommand\algorithmicparfor{\textbf{ParFor}}
\algnewcommand\algorithmicpardo{\textbf{do}}
\algnewcommand\algorithmicendparfor{\textbf{end\ ParFor}}
\algrenewtext{ParFor}[1]{\algorithmicparfor\ #1\ \algorithmicpardo}
\algrenewtext{EndParFor}{\algorithmicendparfor}

% for numbered citations
\usepackage{cite}
% for figures based on pdfs
\usepackage[pdftex]{graphicx}
 
 % Ben's packages
\usepackage{pgfplots}
\pgfplotsset{compat=1.13}
\usepackage{times}
\usepackage{listings}
\graphicspath{ {images/} }


\begin{document}

\title{Enabling Usage of Parallelism In Python}
\author{
\IEEEauthorblockA{Benjamin James Gaska}
\IEEEauthorblockA{Computer Science\\
University of Arizona\\
Tucson, Arizona\\
Email: bengaska@email.arizona.edu}}

\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}

%Paragraph 1, give problem. Scientific researchers like using high-level languages. We must work around their usage

The amount of data available to researchers has exploded in recent years,
whole fields are based in performing large dataset analysis.
Bioinformatics research involves searching through terrabytes of genetic data
to identify meaningful variations.\cite{bolstad2003comparison}
Social sciences are performing analysis of millions of tweets to detect political trends in lieu of traditional surveying methods. 
\cite{cody2016public}
Literary critics are building models of history of the novel through analysis of millions of books over hundreds of years.\cite{moretti2005graphs}
These users are searching for tools that enable them to quickly perform their 
with minimal effort.
The programming language chosen, and the libraries available in the language, 
has the most direct impact upon the user's work.

Several high-level programming languages have become very popular data analysis tasks, in particular SAS, R, and Python.\cite{kdnuggetSurvey}
Python in particular has quickly grown to be one of the most popular general purpose programming languages.
It is one of the most popular languages in use today for data 
analysis, both in raw number of users\cite{kdnuggetSurvey} and rate 
of growth of its user base.\cite{kdnuggetGrowthSurvey}
This popularity comes despite the fact that Python is quite slow.
Runtime comparisons of bioinformatic algorithms
show Python to be, on average, an order of magnitude slower than 
compiled languages such as C/C++ and Java.\cite{fourment2008comparison}
Long runtimes can lead to Python becoming infeasible
for computationally expensive analyses.


ParPython is a way to overcome the runtime bottleneck by enabling
easy usage of parallelism in the language. 
Inspired by OpenMP's pragma-based model\cite{dagum1998openmp}, 
ParPython allows users to specify which loops to parallelize. 
The specified loops are than
transformed to use the parallel tools available in Python. 
This is designed to minimally interfere with the underlying code logic.
Allowing users to place parallelism without having to significantly alter
the serial code.

This paper presents the following contributions: 
\begin{enumerate}
    \item ParPython a novel tool for transformation of serial Python code
    into parallel code,
    \item a survey of research scientist's programming habits, the
    bottlenecks they face and evidence that the ParPython model is 
    preferred over other parallelization options in Python,
    \item comparison of ParPython performance in a variety of 
    common Python benchmarks and libraries,
    \item a case study evaluating the usage of ParPython in
    a real-world planetary science code. 
\end{enumerate}

It is difficult to overcome the slowness of Python relative to
highly performant languages.
What is possible is making Python fast enough that it is not the 
bottleneck for the researcher's work.
This enables users to continue using Python, and the conveniences that 
brings, without excessively impeding the data pipelines that underpin 
modern scientific research.
In this way, we can enable researchers to perform their analysis
faster with less effort, freeing them to do more than they otherwise 
could.

\section{Motivation}

Scientific analysis is often limited by their data processing pipeline.
Across topics as varied as cancer research \cite{danford2016analyzing} and planetary science, (TODO: add arxiv paper)
researchers are working with tools that are they are not able to make performant enough to satisfy their needs.
This forces researchers to limit their work in some way, requiring coarser-grained analysis, analyzing less data than they would prefer,
or even preventing some work from being done at all.
Finding ways to circumvent these issues leads to improvement in
many scientific endeavors.


The Python Standard Library has two modules for parallelism:
threading and multiprocessing, utilizing thread-based and 
process-based parallelism respectively.
The reference implementation of Python, CPython, famously has a 
Global Interpreter Lock (GIL) which ensures that only one thread 
has access to the interpreter at a given
time.\cite{beazley2010understanding}
This limits the performance improvements that can be achieved with
threads.
This necessitates working with heavyweight processes instead.

The multiprocessing module requires the user to implement parallelism 
either as a map or a task-worker queue. 
Both of these abstractions can differ significantly from the for-loop 
structure most familiar for Python users. 
This can require significant refactoring of the serial code, cause 
significant tangling of the parallel logic with the main algorithmic 
logic, and require usage models that can be unfamiliar with many users.

These issues create a significant impediment towards usage of parallelism in Python. 
Even when the problem matches well with the parallel tools available 
users can be discouraged from straying away from their serial 
implementation.
This is supported by survey data which shows that users tend to shy away from parallelism.
(TODO: Need to clarify some things, will extend further with discussion of programmer's tendency to avoid parallelism)


These challenges led us to create a new model that emphasizes ease of use.
By presenting a simple interface to the user, we believe that they will
be more likely to utilize parallelism in situations where they would have
otherwise been discouraged from doing so. 
By taking the job of performing the necessary transformation from
the user we can also apply other techniques to optimize the loop-body.
This allows for further improvements to be made to the code transformation
process, without requiring the users to ever update their code. 


\section{Implementation}

The ParPython toolset is implemented as a single new ParFor object. 
The code's Abstract Syntax Tree (AST) is searched and the ParFor object
is associated with the loop immediately succeeding it.
Further analysis is done to identify what values created outside 
of the for-loop is needed for the computation inside the for-loop.
This information is combined with the information about which variables
are needed, as well as an optional reduction.
Tbe AST is then modfied to parallelize the loop, using the constructs available in the standard library. 
The transformed loop is exactly equivalent to the original serial construct, requiring no modification of any code outside of the 
parallelized loop to ensure correctness of the overall program.

Figure \ref{basicExample} shows a simple example of a singly-nested loop
which process the items in a collection and appends them to a list for 
further use somewhere later in the code.
Parallelization is indicated by the user explicitly by placing a ParFor 
object before the loop to be run in parallel.
The user passes into the ParFor constructor the variables they wish to have returned from the loop body.
Then, before runtime the Abstract Syntax Tree is transformed by ParPython
to produce code that uses the built-in Python parallel constructs.
Arbitrary code can be used within the for-loop body, and there are no
limitations on how the body of the for-loop is written.


%Discuss AST analysis to determine which variables are necessary for 
%computing the output

%Discuss optional reductions

The only difference between the serial implementation and the ParPython implementation is the introduction of the ParFor object.

\begin{figure}[t]
\begin{lstlisting}[frame=single]
ParFor(output=(x))
for i in collection:
    x.append(processValue(i))
\end{lstlisting}
\caption{Simple example of parallelizing a for loop using Parfor. The loop builds up lists x by performing some computation on each value in the given collection of values.}
\label{basicExample}
\end{figure}

\section{Results}

The ParPython implementation strives for simplicity, ease of use, and minimal difference between the serial and parallel code.
To this end, 40 benchmarks were chosen and an attempt was made to parallelize it using only the ParPython parallelism model.

The benchmarks were sampled from a larger set of benchmarks made up of the following:
\begin{itemize}
   \item Python Performance Suite\cite{pyPerformance}, a collection of benchmarks for comparison of Python implementations
   \item Numpy Benchmark Suite\cite{numpyPerformance}, the official test suite for Numpy 
   \item Scikit-Learn Benchmarks \cite{scikit-learn}, the official benchmarks for the popular Python machine learning library
   \item Natural Language Toolkit (NLTK) \cite{bird_2016}, an 
   open-source codebase for natural language processing in Python.
\end{itemize}
These were chosen because they represent a broad variety of code. 
This include implementation entirely in Python, and implementations 
that have a significant portion of its computation performed by calls to external C code.
CPython's native interfacing with C code makes the latter a particularly common usage case.
Further, Numpy, Scikit-Learn, and NLTK represent popular libraries for dataset analysis and scientific programming in Python.


ParPython was evaluated on the code in two ways. The first is how
easy it was to use ParPython with the pre-existing code, as well as 
determining in which use cases ParPython was able to 
parallelize the code at all.
The second takes the code the was able to be parallelized using our method and compares improvements in runtime of code.

\subsection{Ease of Implementation}

The core goal of the ParPython project is as simple an interface for specifying parallelism as possible. To this end, 
the benchmarks were classified into one of three possible categories:
\begin{enumerate}
   \item No modification required beyond inserting ParFor,
   \item some refactoring was required to implement using the ParFor,
   \item or code could not be parallelized using ParFor.
\end{enumerate}



%\begin{tikzpicture}
%\begin{axis}[
%	ylabel=\# of Benchmarks,
%	enlargelimits=0.05,
%	legend style={at={(0.5,-0.1)},
%	anchor=north,legend columns=-1},
%	ybar interval=0.7,
%]
%\addplot 
%	coordinates {(,388950) (2011,393007) 
%		(2010,398449) (2009,395972) (2008,398866)};
%\legend{}
%\end{axis}
%\end{tikzpicture}

\subsection{Performance}

PLACEHOLDER




\section{Related Work}

PLACEHOLDER 
%PyMP\cite{pymp} is a library for Python that allows parallel iterators 
%in Unix environments. 
%It relies on making direct calls to the fork() system 
%call, and is not compatible with non-Unix environments. 
%Further, it does not support reductions and can require 
%non-trivial rewrites of the serial code to fit within the model.

%Danford and Welch\cite{danford2016analyzing} takes a pre-existing


\section{Conclusions}

PLACEHOLDER



\bibliographystyle{IEEEtran}
\bibliography{Thesis}
\end{document}
